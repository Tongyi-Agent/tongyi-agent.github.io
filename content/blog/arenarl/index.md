---
date: '2026-01-14T14:00:00+08:00'
title: '[Tongyi DeepResearch x Amap] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-Based Relative Ranking'
showtoc: true
---

{{< button href="https://huggingface.co/papers/2601.06487" label="PAPER" external=true >}}
{{< button href="https://github.com/Alibaba-NLP/qqr" label="GITHUB" external=true >}}
{{< button href="https://huggingface.co/collections/Alibaba-NLP/arenarl" label="HUGGINGFACE" external=true >}}

Today,Â weÂ areÂ excitedÂ toÂ announceÂ theÂ releaseÂ andÂ open&#8209;sourcingÂ ofÂ **ArenaRL**,Â aÂ novelÂ comparativeÂ reinforcementÂ learningÂ (RL)Â methodÂ designedÂ specificallyÂ forÂ open&#8209;endedÂ agents.Â AlongsideÂ theÂ methodology,Â weÂ areÂ releasingÂ theÂ trainingÂ frameworkÂ andÂ aÂ comprehensiveÂ suiteÂ ofÂ benchmarksÂ forÂ full&#8209;lifecycleÂ evaluation.

AsÂ AIÂ evolvesÂ fromÂ passiveÂ question&#8209;answeringÂ toÂ activeÂ problem&#8209;solving,Â enhancingÂ anÂ agent'sÂ planningÂ andÂ executionÂ capabilitiesÂ throughÂ ReinforcementÂ LearningÂ (RL)Â hasÂ becomeÂ aÂ focalÂ pointÂ inÂ theÂ industry.Â WhileÂ RLÂ hasÂ demonstratedÂ remarkableÂ successÂ inÂ domainsÂ withÂ verifiableÂ outcomesâ€”suchÂ asÂ mathematicsÂ andÂ codeÂ generationâ€”itÂ facesÂ significantÂ hurdlesÂ inÂ open&#8209;endedÂ tasksÂ likeÂ complexÂ travelÂ planningÂ orÂ deepÂ marketÂ research,Â whereÂ noÂ singleÂ "goldenÂ answer"Â exists.

ToÂ addressÂ this,Â weÂ proposeÂ **ArenaRL**.Â ByÂ movingÂ awayÂ fromÂ unstableÂ absoluteÂ scalarÂ scoringÂ andÂ introducingÂ aÂ **TournamentÂ Mechanism**,Â ArenaRLÂ derivesÂ robustÂ rewardÂ signalsÂ throughÂ pairwiseÂ comparisons.Â Crucially,Â itÂ utilizesÂ anÂ optimizedÂ **SeededÂ Single&#8209;Elimination**Â topologyÂ toÂ maintainÂ computationalÂ complexityÂ atÂ aÂ controllableÂ linearÂ levelÂ ($O(N)$),Â strikingÂ theÂ optimalÂ balanceÂ betweenÂ performanceÂ andÂ efficiency.

ThisÂ methodÂ hasÂ alreadyÂ beenÂ validatedÂ inÂ large&#8209;scale,Â real&#8209;worldÂ scenariosÂ withinÂ **Amap'sÂ (GaodeÂ Map)**Â coreÂ businessÂ operations.

# TheÂ Challenge:Â DiscriminativeÂ CollapseÂ inÂ Open&#8209;EndedÂ Tasks

InÂ open&#8209;endedÂ tasksâ€”suchÂ asÂ "PlanÂ aÂ cost&#8209;effectiveÂ familyÂ trip"â€”traditionalÂ RLÂ paradigmsÂ typicallyÂ relyÂ onÂ aÂ RewardÂ ModelÂ toÂ assignÂ anÂ absoluteÂ scalarÂ scoreÂ (PointwiseÂ Scoring)Â toÂ eachÂ generatedÂ trajectory.

However,Â inÂ practice,Â weÂ identifiedÂ aÂ criticalÂ failureÂ modeÂ inÂ thisÂ approach:Â **DiscriminativeÂ Collapse**.

1.  **AbsenceÂ ofÂ ObjectiveÂ GroundÂ Truth:**Â Open&#8209;endedÂ tasksÂ areÂ inherentlyÂ subjective.Â UnlikeÂ mathÂ problems,Â itÂ isÂ difficultÂ forÂ aÂ RewardÂ ModelÂ toÂ assignÂ aÂ precise,Â objectiveÂ absoluteÂ score.
    
2.  **SignalÂ DominatedÂ byÂ Noise:**Â AsÂ theÂ policyÂ modelÂ improves,Â theÂ qualityÂ ofÂ responsesÂ tendsÂ toÂ convergeÂ (e.g.,Â allÂ scoringÂ betweenÂ 0.8Â andÂ 0.9).Â TheÂ RewardÂ ModelÂ strugglesÂ toÂ distinguishÂ subtleÂ differencesÂ betweenÂ high&#8209;qualityÂ responses.Â Consequently,Â randomÂ noiseÂ inÂ theÂ scoringÂ processÂ drownsÂ outÂ theÂ trueÂ advantageÂ signal,Â leadingÂ toÂ stagnationÂ orÂ evenÂ degenerationÂ duringÂ training.
    

{{< figure src="https://cdn.jsdelivr.net/gh/Tongyi-Agent/tongyi-agent.github.io@main/static/img/arenarl/reward.png#center" width="100%">}}

## CoreÂ Method:Â ArenaRLÂ â€”Â RelativeÂ RankingÂ viaÂ Tournaments

ArenaRLÂ introducesÂ aÂ paradigmÂ shift:Â **movingÂ fromÂ absoluteÂ scoringÂ toÂ intra&#8209;groupÂ relativeÂ ranking**.Â WeÂ constructedÂ aÂ multi&#8209;dimensionalÂ evaluationÂ arenaÂ toÂ ensuringÂ theÂ modelÂ receivesÂ robustÂ optimizationÂ signalsÂ evenÂ inÂ complexÂ open&#8209;endedÂ domains.

### 1.Â EvaluationÂ Mechanism:Â Process&#8209;AwareÂ PairwiseÂ Evaluation

TheÂ qualityÂ ofÂ anÂ open&#8209;endedÂ agentÂ dependsÂ notÂ justÂ onÂ theÂ finalÂ answer,Â butÂ onÂ theÂ reasoningÂ process.Â ArenaRLÂ introducesÂ aÂ **Process&#8209;Aware**Â evaluationÂ mechanismÂ thatÂ scrutinizesÂ notÂ onlyÂ theÂ outcomeÂ butÂ alsoÂ theÂ logicalÂ tightnessÂ ofÂ theÂ **Chain&#8209;of&#8209;ThoughtÂ (CoT)**Â andÂ theÂ precisionÂ ofÂ **ToolÂ Use**.

ToÂ mitigateÂ **PositionalÂ Bias**Â whenÂ usingÂ LLMsÂ asÂ judges,Â weÂ employÂ aÂ bidirectionalÂ scoringÂ protocol,Â ensuringÂ thatÂ everyÂ "match"Â yieldsÂ aÂ fairÂ andÂ fine&#8209;grainedÂ result.

### 2.Â CoreÂ Algorithm:Â Tournament&#8209;BasedÂ RelativeÂ Ranking

ArenaRLÂ generatesÂ aÂ groupÂ ofÂ candidateÂ responsesÂ forÂ aÂ singleÂ query,Â creatingÂ anÂ "arena."Â TheÂ modelÂ engagesÂ inÂ self&#8209;play,Â derivingÂ relativeÂ advantageÂ signalsÂ throughÂ pairwiseÂ comparisons.

ThisÂ mechanismÂ reframesÂ rewardÂ modelingÂ asÂ anÂ **intra&#8209;groupÂ relativeÂ ranking**Â problem.Â ThroughÂ quantileÂ rewardÂ mapping,Â discreteÂ rankingsÂ areÂ convertedÂ intoÂ normalizedÂ **AdvantageÂ Signals**.Â ComparedÂ toÂ absoluteÂ scores,Â relativeÂ rankingÂ isÂ inherentlyÂ moreÂ robustÂ toÂ noise,Â capableÂ ofÂ capturingÂ subtleÂ nuancesÂ betweenÂ high&#8209;qualityÂ trajectoriesÂ andÂ effectivelyÂ circumventingÂ "DiscriminativeÂ Collapse."

{{< figure src="https://cdn.jsdelivr.net/gh/Tongyi-Agent/tongyi-agent.github.io@main/static/img/arenarl/method_final.png#center" width="100%">}}

### 3.Â TopologyÂ Optimization:Â FromÂ Round&#8209;RobinÂ toÂ SeededÂ Single&#8209;Elimination

ToÂ findÂ theÂ sweetÂ spotÂ betweenÂ efficiencyÂ andÂ accuracy,Â weÂ systematicallyÂ comparedÂ variousÂ tournamentÂ topologiesÂ inÂ ourÂ paper.Â TheÂ experimentalÂ dataÂ (basedÂ onÂ theÂ Open&#8209;TravelÂ benchmark)Â isÂ shownÂ below

| Topology | ComparisonÂ Cost | Avg.Â WinÂ Rate |
| --- | --- | --- |
| Anchor-Based | $N-1$ | 27.8 |
| Swiss-System | $NÂ \logÂ N$ | 28.3 |
| Double-Elimination | $2N-2$ | 30.2 |
| **SeededÂ Single-Elimination** | $2N-2$ | **32.5** |
| **Round-Robin** | $N(N-1)/2$ | **32.9** |

BasedÂ onÂ theseÂ findings,Â ArenaRLÂ adoptsÂ theÂ **SeededÂ Single&#8209;Elimination**Â architecture:

*   **Anchor&#8209;BasedÂ Seeding:**Â WeÂ useÂ aÂ baselineÂ trajectoryÂ generatedÂ viaÂ greedyÂ decodingÂ asÂ aÂ "qualityÂ anchor"Â toÂ quicklyÂ pre&#8209;rankÂ candidatesÂ andÂ establishÂ seedÂ positions.Â ThisÂ preventsÂ high&#8209;qualityÂ samplesÂ fromÂ "colliding"Â andÂ eliminatingÂ eachÂ otherÂ inÂ earlyÂ rounds.
    
*   **LinearÂ Elimination:**Â AÂ binaryÂ tournamentÂ treeÂ isÂ constructedÂ basedÂ onÂ theÂ seedÂ order.
    

ExperimentsÂ demonstrateÂ thatÂ thisÂ mechanismÂ strictlyÂ controlsÂ computationalÂ complexityÂ atÂ aÂ linearÂ $O(N)$Â levelÂ whileÂ achievingÂ anÂ advantageÂ estimationÂ accuracyÂ thatÂ closelyÂ approximatesÂ theÂ fullÂ Round&#8209;RobinÂ tournament.

# OpenÂ Data:Â Full&#8209;LifecycleÂ Benchmarks

TheÂ communityÂ currentlyÂ lacksÂ RLÂ trainingÂ andÂ evaluationÂ datasetsÂ specificallyÂ tailoredÂ forÂ **Open&#8209;EndedÂ Agents**.Â ToÂ fillÂ thisÂ gap,Â weÂ haveÂ constructedÂ andÂ open&#8209;sourcedÂ twoÂ majorÂ benchmarks:Â **Open&#8209;Travel**Â andÂ **Open&#8209;DeepResearch**.

*   **Open&#8209;TravelÂ (Co&#8209;developedÂ withÂ Amap):**Â DerivedÂ fromÂ real&#8209;worldÂ travelÂ scenarios,Â coveringÂ 5Â typicalÂ tasksÂ suchÂ asÂ vagueÂ intentÂ understanding,Â multi&#8209;waypointÂ planning,Â andÂ spatiotemporalÂ constraintÂ trade&#8209;offs.Â ItÂ replicatesÂ theÂ complexÂ decision&#8209;makingÂ environmentÂ ofÂ "multipleÂ constraints,Â noÂ standardÂ solution."
    
*   **Open&#8209;DeepResearch:**Â FocusesÂ onÂ long&#8209;horizonÂ informationÂ retrievalÂ andÂ reportÂ generation,Â evaluatingÂ theÂ agent'sÂ abilityÂ inÂ multi&#8209;stepÂ toolÂ invocation,Â informationÂ verification,Â andÂ deepÂ synthesis.
    

ToÂ supportÂ theÂ reproductionÂ ofÂ theÂ ArenaRLÂ trainingÂ pipeline,Â weÂ haveÂ releasedÂ theÂ **TestÂ Sets**Â andÂ theÂ **RLÂ TrainingÂ SetsÂ (Prompts)**Â forÂ theseÂ benchmarks:

| Dataset | Domain | RLÂ TrainingÂ Set | TestÂ Set |
| --- | --- | --- | --- |
| **Open-Travel** | ComplexÂ TravelÂ Planning | 1,626 | 250 |
| **Open-DeepResearch** | DeepÂ InfoÂ Retrieval | 2,216 | 100 |
| Total |  | **3,842** | **350** |

# ExperimentalÂ Results

WeÂ conductedÂ extensiveÂ evaluationsÂ onÂ Open-Travel,Â Open-DeepResearch,Â andÂ generalÂ writingÂ tasks.Â AsÂ shownÂ below,Â ArenaRLÂ achievesÂ significantÂ performanceÂ advantagesÂ overÂ SFTÂ baselinesÂ andÂ traditionalÂ RLÂ methodsÂ likeÂ GRPOÂ andÂ GSPO:

| Method | Open-Travel | Open-DeepResearch | GeneralÂ Writing |
| --- | --- | --- | --- |
| Closed-sourceÂ Models |  |  |  |
| GPT-4o | 2.6 | 12.2 | 76.0 |
| Grok-4 | 16.8 | 34.8 | 84.7 |
| Gemini-2.5-pro | 10.6 | 28.3 | 85.4 |
| Claude-3.7-Sonnet | 31.6 | 19.1 | 76.6 |
| Fine-tuningÂ &Â RL |  |  |  |
| SFT | 16.4 | 16.7 | 72.1 |
| GRPO | 16.4 | 25.2 | 73.6 |
| GSPO | 17.2 | 25.2 | 73.0 |
| **ArenaRLÂ (Ours)** | **41.8** | **64.3** | **80.3** |

**Analysis:**

1.  **ComplexÂ PlanningÂ (Open&#8209;Travel):**Â InÂ tasksÂ involvingÂ vagueÂ intentsÂ andÂ multi&#8209;dimensionalÂ constraints,Â ArenaRLÂ achievedÂ aÂ **significantÂ performanceÂ boost**Â comparedÂ toÂ SFTÂ andÂ traditionalÂ RL.Â ThisÂ demonstratesÂ thatÂ theÂ tournamentÂ mechanismÂ effectivelyÂ incentivizesÂ theÂ modelÂ toÂ escapeÂ localÂ optimaÂ andÂ exploreÂ superiorÂ planningÂ strategies.
    
2.  **Long&#8209;HorizonÂ TasksÂ (Open&#8209;DeepResearch):**Â TraditionalÂ RLÂ methodsÂ oftenÂ produceÂ unusableÂ outputsÂ dueÂ toÂ "lengthÂ bias"Â inÂ long&#8209;contextÂ tasks.Â ArenaRLÂ improvedÂ theÂ **ValidÂ GenerationÂ RateÂ toÂ ~99%**,Â effectivelyÂ solvingÂ theÂ instruction&#8209;followingÂ challengeÂ inÂ long&#8209;textÂ scenarios.
    
3.  **GeneralÂ Writing:**Â OnÂ threeÂ majorÂ generalÂ writingÂ benchmarks,Â ArenaRLÂ alsoÂ performedÂ exceptionallyÂ well,Â provingÂ thatÂ theÂ methodÂ possessesÂ strongÂ generalizationÂ capabilitiesÂ beyondÂ tool&#8209;useÂ agents.
    

## Real&#8209;WorldÂ Application:Â AmapÂ (GaodeÂ Map)

ArenaRLÂ hasÂ notÂ onlyÂ excelledÂ inÂ academicÂ benchmarksÂ butÂ hasÂ alsoÂ beenÂ successfullyÂ validatedÂ inÂ **Amap'sÂ real&#8209;worldÂ businessÂ scenarios**.Â WeÂ evaluatedÂ itÂ acrossÂ twoÂ dimensions:Â DeterministicÂ SearchÂ andÂ Open&#8209;EndedÂ Planning.

*   **DeterministicÂ POIÂ Search:**Â InÂ POIÂ searchÂ tasksÂ definedÂ byÂ explicitÂ rulesÂ andÂ intenseÂ competition,Â ArenaRLÂ demonstratedÂ extremeÂ adaptabilityÂ toÂ rigidÂ constraints.Â ComparedÂ toÂ theÂ baseline,Â searchÂ accuracyÂ improvedÂ byÂ **75%Â toÂ 83%**.Â ThisÂ provesÂ thatÂ evenÂ inÂ deterministicÂ settings,Â theÂ tournamentÂ mechanismÂ canÂ keenlyÂ captureÂ subtleÂ qualityÂ differences,Â pushingÂ performanceÂ beyondÂ existingÂ bottlenecks.
    
*   **ComplexÂ Open&#8209;EndedÂ Planning:**Â AddressingÂ multi&#8209;stepÂ reasoningÂ tasksâ€”suchÂ asÂ _"FindÂ aÂ quietÂ barÂ nearÂ theÂ BundÂ withÂ aÂ river&#8209;viewÂ terraceÂ forÂ aÂ date"_Â orÂ _"Inter&#8209;cityÂ travelÂ withÂ luggageÂ andÂ timeÂ constraints"_â€”theÂ coreÂ businessÂ metricÂ roseÂ fromÂ **69%Â toÂ 80%**.Â TheÂ modelÂ exhibitedÂ strongerÂ logicalÂ consistency,Â effectivelyÂ handlingÂ vagueÂ userÂ intentsÂ andÂ makingÂ optimalÂ trade&#8209;offsÂ betweenÂ multipleÂ constraintsÂ (time,Â cost,Â preference),Â significantlyÂ enhancingÂ userÂ satisfactionÂ inÂ complexÂ tailÂ scenarios.
    

# SystemÂ ArchitectureÂ &Â Ecosystem

ToÂ empowerÂ theÂ developerÂ community,Â weÂ haveÂ simultaneouslyÂ open&#8209;sourcedÂ theÂ RLÂ training/testÂ dataÂ andÂ theÂ `qqr`Â trainingÂ framework.Â `qqr`Â isÂ aÂ lightweight,Â non&#8209;intrusiveÂ extensionÂ libraryÂ builtÂ onÂ [`slime`](https://github.com/THUDM/slime),Â designedÂ specificallyÂ forÂ open&#8209;endedÂ agentÂ training.

*   **ArenaRLÂ Algorithm:**Â FullÂ implementationÂ ofÂ theÂ coreÂ algorithmsÂ describedÂ inÂ theÂ paper.Â ItÂ includesÂ built&#8209;inÂ topologiesÂ forÂ Anchor&#8209;Based,Â Round&#8209;Robin,Â Swiss&#8209;System,Â Double&#8209;Elimination,Â andÂ SeededÂ Single&#8209;EliminationÂ tournaments.
    
*   **DesignedÂ forÂ Open&#8209;EndedÂ Agents:**Â SpecificallyÂ engineeredÂ toÂ tackleÂ discriminativeÂ collapseÂ inÂ complex,Â open&#8209;endedÂ tasks,Â ensuringÂ continuousÂ policyÂ improvementÂ viaÂ relativeÂ rankingÂ evenÂ whenÂ rewardÂ modelÂ scoresÂ stagnate.
    
*   **MCPÂ Support:**Â SeamlesslyÂ integrationÂ withÂ theÂ MCPÂ standardizesÂ theÂ decouplingÂ ofÂ LLMÂ inferenceÂ andÂ toolÂ environments.Â DevelopersÂ canÂ reuseÂ existingÂ MCPÂ ServersÂ asÂ trainingÂ environmentsÂ withoutÂ rewritingÂ interfaces.
    
*   **High&#8209;PerformanceÂ Training:**Â BuiltÂ onÂ topÂ ofÂ [`slime`](https://github.com/THUDM/slime)toÂ deliverÂ high&#8209;throughput,Â distributedÂ rolloutÂ generationÂ andÂ trainingÂ forÂ large&#8209;scaleÂ agentÂ evolution.
    

WeÂ hopeÂ ArenaRLÂ providesÂ theÂ communityÂ withÂ aÂ practicalÂ methodologyÂ forÂ agentÂ evolution,Â propellingÂ usÂ fromÂ ImitationÂ LearningÂ towardÂ aÂ broaderÂ eraÂ ofÂ **Self&#8209;Evolution**.

## ğŸ”—Â OpenÂ SourceÂ Links

*   **Paper:**Â [https://huggingface.co/papers/2601.06487](https://huggingface.co/papers/2601.06487)
    
*   **GitHub:**Â [https://github.com/Alibaba-NLP/qqr](https://github.com/Alibaba-NLP/qqr)
    
*   **HuggingFace**:Â [https://huggingface.co/collections/Alibaba-NLP/arenarl](https://huggingface.co/collections/Alibaba-NLP/arenarl)