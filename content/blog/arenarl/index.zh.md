---
date: '2026-01-14T14:00:00+08:00'
title: '通义DeepResearch x 高德地图联合开源 ArenaRL：面向开放域智能体的对比式强化学习方法'
showtoc: true
---

{{< button href="https://huggingface.co/papers/2601.06487" label="PAPER" external=true >}}
{{< button href="https://github.com/Alibaba-NLP/qqr" label="GITHUB" external=true >}}
{{< button href="https://huggingface.co/collections/Alibaba-NLP/arenarl" label="HUGGINGFACE" external=true >}}

今日，通义 DeepResearch 团队与高德地图联合发布并开源 ArenaRL。这是一套专为开放域智能体设计的对比式强化学习方法。同时，我们配套开源了训练框架及全流程评测基准。

随着人工智能从被动问答向主动问题解决的范式演进，如何通过强化学习（RL）提升智能体在复杂环境中的规划与执行能力，已成为业界焦点。在数学、代码等有明确标准答案的任务中，强化学习已取得了显著成果。

然而，面对旅行规划、深度研报撰写等开放域任务，由于缺乏唯一的正确答案，传统的强化学习方法往往陷入瓶颈。

针对这一挑战，我们提出 ArenaRL。它跳出了传统绝对打分的桎梏，创新性地引入锦标赛机制——该机制在通过成对比较获取高鲁棒性奖励信号的同时，利用高效的淘汰赛拓扑结构，将计算复杂度控制在可控的线性水平，避免了算力爆炸，实现了效果与效率的最佳平衡。目前，该方法已在高德地图核心业务场景完成落地验证。

ArenaRL 究竟如何打破这一技术瓶颈？下文将深度解析其背后的核心算法与运行机制：

# 技术挑战：开放域任务中的判别崩溃

在诸如“规划一次适合亲子游且性价比高”的开放域任务中，传统的强化学习范式通常依赖奖励模型对每个生成的轨迹打出一个绝对标量分数。

但在实际训练中，我们发现这种方法存在一个致命缺陷——判别崩溃：

1.  **缺乏客观标准答案：** 开放任务主观性强，奖励模型难以像数学题那样给出精准的绝对分值。
    
2.  **信号淹没：** 随着策略模型能力提升，生成的回答质量普遍提高（例如都在 0.8-0.9 分之间），奖励模型难以区分高水平回答之间的细微优劣。此时，打分中的随机噪声会盖过真实的优势信号，导致训练优化停滞甚至模型退化。
    

{{< figure src="https://cdn.jsdelivr.net/gh/Tongyi-Agent/tongyi-agent.github.io@main/img/arenarl/reward.png#center" width="100%">}}

# 核心方法：ArenaRL —— 基于锦标赛机制的相对排序

为了解决上述问题，ArenaRL 提出了一种范式转变：从绝对打分转向组内相对排序。我们构建了一个多维度的评价与竞技体系，确保模型在复杂的开放任务中能持续获得稳健的优化信号。

## 评价机制：过程感知成对评估

开放域智能体的优劣不仅取决于最终答案，更取决于推理规划的过程。ArenaRL 引入了过程感知的评估机制，不仅对比最终结果的质量，更深入审视思维链（CoT）的逻辑严密性以及工具调用的精准度。

同时，为了消除大模型作为裁判时的位置偏见，我们采用了双向评分协议，确保每一场对决的评估结果都是公正且细粒度的。

## 核心算法：锦标赛相对排序

ArenaRL 让智能体针对同一指令生成一组候选方案，构建一个竞技场。模型在自我博弈中通过成对比较获取相对优势信号。

这种机制将奖励建模重构为组内相对排序问题，并通过分位奖励映射将离散排名转化为归一化的优势信号。相比于绝对分数，相对排序天然更能抵抗噪声，敏锐捕捉高质量轨迹间的细微差异，有效规避了训练后期的判别崩溃。

{{< figure src="https://cdn.jsdelivr.net/gh/Tongyi-Agent/tongyi-agent.github.io@main/img/arenarl/method_final.png#center" width="100%">}}

## 拓扑结构优化：从全量循环到种子单败淘汰

为了寻找效率与准确率的最佳平衡点，我们在论文中系统性地对比了多种竞技拓扑，实验数据（基于 Open-Travel 基准）如下表所示：

| 拓扑结构 | 比较开销 | 平均胜率 |
| --- | --- | --- |
| 基于锚点的排序 (Anchor-Based) | $N-1$ | 27.8 |
| 瑞士轮 (Swiss-System) | $N \log N$ | 28.3 |
| 双败淘汰 (Double-Elimination) | $2N-2$ | 30.2 |
| **种子单败淘汰 (Seeded Single-Elimination)** | $2N-2$ | **32.5** |
| **全量循环赛 (Round-Robin)** | $N(N-1)/2$ | **32.9** |

*   **全量循环赛（Round-Robin）：** 让所有候选方案两两对决。这是评估的理想基准，但$O(N^2)$指数级的计算复杂度使其无法应用于大规模在线训练。
    
*   **瑞士轮（Swiss System）与双败淘汰（Double-Elimination）：** 这类赛制虽比全量循环快，但仍存在局限：瑞士轮复杂度较高（$O(N \log N)$）；而双败淘汰虽接近线性复杂度，但在缺乏高质量种子预排序时，容易受随机性影响，排序保真度不足。
    
*   **基于锚点的排序（Anchor-Based Ranking）：** 虽然也将复杂度降低至$O(N)$，但它仅将所有候选方案与一个固定的基准（Anchor）进行单点比较。这种方式缺乏候选方案之间的直接博弈，难以分辨高质量方案之间的细微优劣，即论文所述的分辨率缺失（Loss of Resolution），导致排序分辨率受限。
    

针对上述痛点，ArenaRL 最终确立了**种子单败淘汰赛**（Seeded Single-Elimination）架构：

*   **锚点预排序：** 引入贪婪解码生成的基准轨迹作为质量锚点，对候选样本进行快速初筛预排序，确立种子顺位。这有效防止了高质量样本在早期轮次中意外撞车。
    
*   **线性淘汰：** 基于种子顺位进行二叉树式的单败淘汰赛。
    

实验证明，该机制在将计算复杂度严格控制在$O(N)$线性水平的同时，其优势估计准确率能够高度逼近全量循环赛，实现了训练效率与效果的最佳平衡。

# 开源数据

当前社区缺乏专门面向**开放域智能体**的 RL 训练与评测数据集。为此，我们构建并开源了 **Open-Travel** 和 **Open-DeepResearch** 两大基准：

*   **Open-Travel（联合高德构建）：** 源自真实出行场景，包含模糊意图理解、多途经点规划、时空约束权衡等 5 类典型任务，完整复刻了“多约束、无标准解”的复杂决策环境。
    
*   **Open-DeepResearch：** 聚焦长链路信息检索与研报生成，考察智能体在多步工具调用、信息去伪存真及深度整合方面的能力。
    

为了支持社区复现 ArenaRL 的训练流程，我们开源了上述基准中的测试集以及 RL 训练集，具体数据规模如下：

| 数据集 | 领域 | RL 训练集 | 测试集 |
| --- | --- | --- | --- |
| **Open-Travel** | 复杂出行规划 | 1,626 | 250 |
| **Open-DeepResearch** | 深度信息检索 | 2,216 | 100 |
| **总计** |  | **3,842** | **350** |

# 实验结果

我们在 Open-Travel、Open-DeepResearch 以及通用写作任务上进行了广泛的评测。如下表所示，相比于 SFT 基线以及 GRPO、GSPO 等传统强化学习方法，ArenaRL 均取得了显著的性能优势：

| 方法 | Open-Travel | Open-DeepResearch | 通用写作 |
| --- | --- | --- | --- |
| Closed-source Models |  |  |  |
| GPT-4o | 2.6 | 12.2 | 76.0 |
| Grok-4 | 16.8 | 34.8 | 84.7 |
| Gemini-2.5-pro | 10.6 | 28.3 | 85.4 |
| Claude-3.7-Sonnet | 31.6 | 19.1 | 76.6 |
| Fine-tuning & RL |  |  |  |
| SFT | 16.4 | 16.7 | 72.1 |
| GRPO | 16.4 | 25.2 | 73.6 |
| GSPO | 17.2 | 25.2 | 73.0 |
| **ArenaRL (Ours)** | **41.8** | **64.3** | **80.3** |

**结果分析：**

1.  **复杂规划任务 (Open-Travel)：** 在包含模糊意图与多维时空约束的出行规划任务中，ArenaRL 相比 SFT 基线及传统 RL 方法实现了显著的性能提升，表明锦标赛机制能有效激励模型跳出局部最优，探索更优的规划策略。
    
2.  **长链路任务 (Open-DeepResearch)：** 面对长文本研报任务，传统 RL 方法容易因长度偏差导致生成不可用。ArenaRL 将有效生成率显著提升，改善了长文本任务中的指令遵循难题，大幅提升了生成结果的可用性。
    
3.  **通用写作任务：** 在三大通用写作榜单上，ArenaRL 同样表现优异，证明该方法在非工具调用型的通用生成任务上也具备良好的泛化能力。
    

## 业务落地验证

ArenaRL 不仅在学术基准上表现优异，更关键的是，它已在高德地图真实业务场景中完成落地验证。我们从确定性搜索与开放式规划两大维度进行了评估：

*   **确定性 POI 搜索：** 在规则明确且竞争激烈的 POI 搜索任务中，ArenaRL 展现了对刚性约束的极强适应力。相比基线，核心业务准确率从 **75% 提升至 83%**。这一结果有力证明，即使在规则确定的场景下，锦标赛机制也能敏锐捕捉高质量结果之间的细微优劣，从而推动模型性能突破瓶颈。
    
*   **复杂开放式规划：** 面对涉及多步推理的开放式行程规划——例如寻找外滩带江景露台且适合约会的酒吧，或北京往返天津、需兼顾行李与时间的跨城出行等复杂需求，业务核心指标从 **69% 提升至 80%**。模型展现出了更强的逻辑自洽性，能够有效处理模糊的用户意图并在多重约束（如时间、成本、偏好）之间做出最优权衡，提升了复杂长尾场景下的用户满意度。
    

# 系统架构与开源生态

为了降低开发者的使用门槛，我们同步开源了 RL 训练与测试数据及配套训练框架 `qqr`。这是一个基于 [`slime`](https://github.com/THUDM/slime)构建的轻量级、非侵入式扩展库，专为开放域智能体的训练而设计。

*   **ArenaRL 算法**: 完整实现了论文中的核心算法。框架内置了锚点法 (Anchor-Based)、循环赛 (Round-Robin)、瑞士轮 (Swiss-System)、双败淘汰 (Double-Elimination) 和种子单败淘汰制 (Seeded Single-Elimination) 等多种锦标赛拓扑。
    
*   **为开放域智能体设计**: 为解决复杂开放域任务中的判别崩溃问题而设计，即使在奖励模型打分趋于同质化的情况下，依然能通过相对排序驱动策略持续改进。
    
*   **MCP 支持**: 集成 MCP 以标准化本地或远程工具的连接，实现了 LLM 推理与工具环境的解耦。开发者可以直接复用现有的 MCP Servers 作为训练环境，无需重写接口。
    
*   **高性能分布式训练：** 底层基于 [`slime`](https://github.com/THUDM/%60)构建，支持大规模智能体进化所需的高吞吐量分布式生成与训练能力。
    

我们希望 ArenaRL 能为社区提供一套切实可用的智能体进化方法论，推动智能体从模仿学习迈向更广阔的自我进化（Self-Evolution）时代。

## 开源传送门

*   **Paper：**[https://huggingface.co/papers/2601.06487](https://huggingface.co/papers/2601.06487)
    

*   **GitHub：** [https://github.com/Alibaba-NLP/qqr](https://github.com/Alibaba-NLP/qqr)
    
*   **HuggingFace**: [https://huggingface.co/collections/Alibaba-NLP/arenarl](https://huggingface.co/collections/Alibaba-NLP/arenarl)